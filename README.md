# Web Scraping Application

Проект представляет собой систему автоматизированного скрейпинга различных веб-сайтов с последующим сохранением данных в JSON-файлы.

## Что было сделано

1. Разработана абстрактная структура `Scraper` для базового функционала скрейпинга.
2. Создан конкретный скрипт-скрапер `QuotesScraper` для извлечения цитат с сайта [quotes.toscrape.com.](https://quotes.toscrape.com/)
3. Реализован `SiteManager`, который управляет регистрацией и координацией различных скриптов-скраперов. *(если такие планируются в дальнейшем)*
4. Написан основной запускаемый скрипт (`starter.py`) для демонстрации работы системы.

## Источники данных

Основной источник данных для этого проекта - сайт quotes.toscrape.com. Код поддерживает получение данных, начиная с конкретной страницы сайта.

## Процесс сборки данных

1. Скрипт-скрапер (`QuotesScraper`) отправляет HTTP-запрос к указанному URL *(По умолчанию https://quotes.toscrape.com/)*.
2. **BeautifulSoup** используется для парсинга HTML-документа и извлечения необходимой информации.
3. Извлеченные данные сохраняются в JSON-файлы с уникальными именами, основанными на имени сайта и дате.
4. `SiteManager` координирует процесс скрейпинга, регистрирует различные скрипты-скраперы и управляет их выполнением.

## Выбор методов и инструментов

1. **Модульная структура проекта**:
    - Выбор: Разделение кода на отдельные файлы обеспечивает чистоту и поддерживаемость кода.
    - Преимущества:
      - Улучшает организацию кода и его читаемость.

   Почему модульная структура и не другая:
    - Сравнение с монолитной структурой: модульная структура более масштабируема и гибкая.
    - Сравнение с микросервисной архитектурой: для текущего проекта такая структура избыточна. 
   
2. **Абстрактный класс Scraper**:
    - Выбор: Абстрактный класс обеспечивает гибкость и расширяемость системы.
    - Преимущества:
      - Позволяет легко добавлять новые типы скриптов-скраперов без изменения существующего кода.
      - Обеспечивает единообразие интерфейса для всех скраперов.
      - Упрощает тестирование и отладку отдельных компонентов.

3. **BeautifulSoup**:
    - Выбор: BeautifulSoup - мощный и легковесный инструмент для парсинга HTML.
    - Преимущества:
      - Легко читаемый и понятный синтаксис.
      - Хорошо подходит для извлечения структурированной информации из HTML-документов.
      - Поддерживает различные форматы входных данных (HTML, XML, JSON).
      - Встроенные функции для поиска элементов по различным критериям.
      - Более быстрое выполнение по сравнению с другими библиотеками для парсинга HTML.

    Почему BeautifulSoup и не другие инструменты:
      - Сравнение с _lxml_: BeautifulSoup проще в использовании и требует меньше настройки.
      - Сравнение с _requests-html_: BeautifulSoup более эффективен при работе с большими объемами данных.
      - Сравнение с _pyquery_: BeautifulSoup предоставляет более удобные методы для работы с DOM-структурой.

4. **Requests**:
    - Выбор: Стандартный HTTP-запрос обеспечивает надежное соединение с веб-серверами.
    - Преимущества:
      - Гарантирует корректную работу скрипта даже при изменениях структуры сайта.
      - Поддерживает все стандартные HTTP-методы (GET, POST, HEAD, etc.).

    Почему HTTP-запросы и не другие инструменты:
      - Сравнение с _urllib.request_: requests предоставляет более удобный API


